\documentclass[a4page, 11pt]{article} %this is behaving like \documentclass[11pt]{article}
% sì, a4page è sottinteso

%\documentclass[a4paper, 11pt]{article} %maybe you want this?

\usepackage[italian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xfrac}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.2in]{geometry}
\usepackage{cancel}


\usepackage{amsthm}
\newtheorem*{remark}{Osservazione}

\usepackage{enumitem} %resuming enumerations
\usepackage{hyperref} %clickable TOC

\def\dep{\not\!\perp\!\!\!\perp}  % simbolo di dipendenza
\def\indep{\perp \!\!\! \perp }   % simbolo di indipendenza

\title{Statistical Modeling}
\author{}
\date{}



\begin{document}
\maketitle
\tableofcontents % magari risulta più facile da consultare per ripassi... @momo
\newpage

\section{Errori eteroschedastici}
Per ottenere stime efficienti per i parametri del modello lineare classico, gli errori devono essere \textit{omoschedastici}: la varianza degli errori $\varepsilon_i$ è costante $Var(\varepsilon_i) = E(\varepsilon_i^2) = \sigma^2$ e non dipende dal valore delle variabili indipendenti $E(\varepsilon_i) = E(\varepsilon_i | x_i)$.
Questo si verifica dal momento che il valore atteso del singolo errore è nulla $E(\varepsilon_i) = 0$.
Quando ciò non avviene, si è in presenza di errori \textit{eteroschedastici}: $Var(\varepsilon_i) = \sigma_i^2$; in tal caso il vettore $b$ dei parametri perde in efficienza\footnote{Si veda dimostrazione in appendice.} (non è più BLUE (\textit{Best Linear Unbiased Estimator}).
In forma matriciale, questo significa che si passa da una matrice diagonale con un valore costante a un'altra matrice diagonale i cui valori possono differire.

% Affinché il nostro modello lineare classico ottenga delle stime efficienti abbiamo bisogno di verificare che gli errori siano omoschedastici. 
% In virtù del fatto che ogni errore $\varepsilon_i$ è a media nulla ($E(\varepsilon_i) = 0$) vale la relazione $E(\varepsilon_i^2) = Var(\varepsilon_i) = \sigma^2$, e quindi gli errori sono detti \textit{omoschedastici} quando la loro varianza è costante al variare del valore dei regressori.
% Se ciò non accade gli errori si dicono \textit{eteroschedastici} $Var(\varepsilon_i) = \sigma_i^2$; questo incide sulle proprietà degli stimatori OLS: in particolare continuano a valere correttezza e consistenza ma viene meno l’efficienza\footnote{Vedi Appendice per la dimostrazione}.
% (lo stimatore non è più \textbf{BLUE}, \textbf{B}est \textbf{L}inear \textbf{U}nbiased \textbf{E}stimator).
Inoltre, la stima campionaria $s^2$ di $\sigma^2$ tende a sottostimare il vero valore della distribuzione, dato che non si è più in presenza di una sola variabile casuale ma di molteplici.
Ne consegue che il test $T$ di Student ritorna valori erroneamente elevati, e quindi gli intervalli di confidenza risultano essere più stretti del reale, e i test di significatività sui parametri $b_j$ risultano più permissivi del dovuto.
Discorso analogo vale per i test basati sulla distribuzione $F$ di Snedcor.

% Inoltre le stime campionarie tendono a sottostimare il vero valore della varianza e non esiste più un’unica varianza, ma ce ne sono molteplici. 
% Come conseguenza la statistica T di Student ha valori erroneamente elevati ed anche i relativi intervalli di confidenza risulteranno più stretti mentre la regione di rifiuto del test T risulterà erroneamente più ampia; verranno quindi ritenuti significativi i parametri anche quando in realtà non lo sono.
% Per individuare questa caratteristica, che ci porta ad un’inaffidabilità delle stime, possiamo ricorrere a diversi metodi (grafici o analitici).
%Per quanto riguarda i metodi grafici:
Si può verificare se una distribuzione è o eteroschedastica (o omoschedastica) tramite diversi metodi grafici:
\begin{itemize}[noitemsep]
  \item scatter plot della variabile risposta vs esplicative $y \sim x_j$ (da effettuare per ogni $x_j$);
    % della variabile target $(y)$ contro le variabili esplicative $x_j$. Ovviamente bisognerà effettuare uno scatterplot per ogni variabile $x_j$;
  \item scatter plot dei valori stimati vs residui $\hat{y} \sim \varepsilon$;
%  \item Scatter plot dei valori predetti ($\hat{y}$) contro i residui stimati ($y - \hat{y} = \varepsilon$);
  \item scatter plot dei residui al quadrato vs i valori predetti $\varepsilon^2 \sim \hat{y}$;
  \item scatter plot dei valori osservati vs predetti $y \sim \hat{y}$;
  \item scatter plot dei residui vs variabili esplicative $\varepsilon \sim x_j$ (da effettuare per ogni $x_j$).
\end{itemize}
Esistono inoltre una serie di test statistici che offrono risultati numerici e meno interpretativi per verificare l'eteroschedasticità degli errori.
% Possiamo ricorrere anche ad alcuni test (metodo analitico):

%\begin{itemize}
\paragraph{Testi di White. } Si basa sull’assunzione di omoschedasticità dei residui ($H_0: Var(\varepsilon_i) = \sigma^2$ contro $H_1: Var(\varepsilon_i) \neq \sigma^2$). % non è = \sigma_i perché è un'affermazione più forte: White l'ipotesi alternativa dice solamente che non è vera la nulla - @momo
%; viene perciò definita l’ipotesi nulla come $H_0:$ Var$(\varepsilon_i) = (\sigma^2)$ e l’ipotesi alternativa come $H_1:$ Var$(\varepsilon_i ) = ( \sigma^2_i )$.
Il test sfrutta la regressione $OLS$ del quadrato dei residui con le variabili esplicative, il loro quadrato e tutte le possibili interazioni $\varepsilon^2 \sim x_j, x_ix_j \hspace{5pt} \forall i, j < k$. %, i regressori al quadrato $x_j^2$ e le loro interazioni.
Attraverso l'indice di determinazione $R^2$ di tale regressione, ricavato dal rapporto tra la variabilità spiegata dalla regressione e la variabilità totale $R^2 = \frac{SSE}{TSS} = \frac{\sum(\hat{y}_i - y_i)^2}{\sum(y_i - \overline{y})^2}$, si calcola la statistica $LM = n R^2 \sim \chi^2_{k-1}$.
L’ipotesi nulla verrà rigettata se $LM$ risulterà maggiore del valore soglia della distribuzione $\chi^2$ (ovvero con p-value basso); %infatti se $R^{2}$ è oltre ad un certo valore significa che le variabili esplicative sono ancora significative nello spiegare i residui al quadrato e quindi che gli errori sono omoschedastici.
%@Dario: E` giusta l'ultima frase? Forse sono io che non capisco, oppure è di dubbia interpretazione: la parte "sono ancora significative nello spiegare..." vuole dire che effettivamente spiegano o il contrario?
%io avrei messo:
infatti se $R^2$ è maggiore di un certo valore soglia significa che le variabili esplicative sono realmente significative nello spiegare la variabilità dei residui.
% , ovvero che i residui al quadrato dipendono dai valori delle variabili esplicative $x_j$, come accade tipicamente in presenza di eteroschedasticità.
% secondo me è più chiara, ma la seconda parte mi sembra ridondante
\begin{equation*}
  LM = nR^2 = n\frac{\sum(\hat{y}_i - y_i)^2}{\sum(y_i - \overline{y})^2} \sim \chi^2_{k-1}
\end{equation*}

\paragraph{Test di Breusch-Pagan.} Anche in questo test, l’ipotesi nulla è quella di omoschedasticità ($H_0: Var(\varepsilon_i) = \sigma^2$).
Il test si basa sulla regressione di $\sfrac{\varepsilon^2_i}{s^2}$ (dove $s^2 = \frac{1}{n}\sum{\varepsilon_i^2}$) con le variabili esplicative.
Della regressione si calcola poi il coefficiente $R^2$ analogamente al test di White.
Essendo  $\varepsilon$ distribuita (sotto $H_0$) come una normale $N(0, \sigma^2)$, $\varepsilon^2 \sim \chi^2_{k-1}$; inoltre essendo già $s^2 \sim \chi^2_{n-k-1}$ (perché anch'esso somma di normali al quadrato) e $\varepsilon \indep X$ (sempre sotto $H_0$), $\sfrac{\varepsilon^2}{s^2}$ si distribuisce come un rapporto di $\chi^2$ indipendenti tra di loro, ovvero come una $F_{k-1, n-k-1}$ di Snedecor.
% La somma dei quadrati dei regressori e quella degli scarti si distribuiscono come $\chi^{2}$ indipendenti. Essendo $\varepsilon$ distribuita normalmente, la sua versione al quadrato $\varepsilon_i^2$ si distribuisce come una $\chi^2$; $s^2$ allo stesso modo in quanto somma di normali al quadrato assume una distribuzione $\chi^2$. A questo punto il loro rapporto si distribuisce quindi come una $F$ di Snedecor, in quanto rappporto tra due $\chi^2$.
Si procede quindi effettuando un semplice test $F$ per l'accettazione di $H_0$.
%L’ipotesi nulla verrà rigettata quando la statistica $F$ è superiore ad un valore soglia.
Per risolvere tale problema si può procedere attraverso il metodo di stima $WLS$ (Weighted Least Squares).
% \end{itemize}
\begin{equation*}
  BP = nR^2 \sim F_{k-1, n-k-1}
\end{equation*}

\section{Errori autocorrelati}
Per garantire stime efficienti del modello, bisogna verificare che gli errori $\varepsilon$ non siano tra di loro correlati, cioè non siano \textit{autocorrelati}.
Nel modello lineare classico si ipotizza infatti che:
\begin{equation*}
  Cov(\varepsilon_i, \varepsilon_j) = E(\varepsilon_i \varepsilon_j) = 0 \hspace{15pt} i \neq j
\end{equation*}
% Affinché il nostro modello lineare classico ottenga delle stime efficienti abbiamo bisogno di verificare che gli errori non siano correlati tra di loro.
Tuttavia accade spesso, soprattutto in serie storiche o territoriali, che esista una correlazione tra errori in momenti successivi o territori vicini.
Gli errori correlati si possono scindere in due componenti: %$\varrho \cdot
$\varepsilon_{i.-1}^\#$ (errore ritardato di un tempo) e $\eta_i$ (errori omoschedastici IID, ovvero indipendentemente ed identicamente distribuiti in modo normale).
Una parte dell'errore è legata al suo valore ritardato, mentre l'altra è indipendente.
% Si nota infatti che l’errore è legato al suo valore ritardato.
Possiamo classificare l’autocorrelazione in base al suo \textit{grado}: si dice autocorrelazione di primo grado quando gli errori sono correlati con il loro valore ritardato di un tempo; allo stesso modo si dice autocorrelazione di $i$-esimo grado quando gli errori sono correlati con il loro valore ritardato di $i$ gradi ($\varrho_{-i}$).
Gli errori autocorrelati non incidono sulle proprietà di linearità, correttezza e consistenza degli stimatori OLS (analogamente agli errori eteroschedastici), ma solo sull’efficienza (non sono più BLUE).
Come nel caso dell’eteroschedasticità, la stima della varianza dei parametri e relative inferenze non sono più corrette e affidabili (la statistica $T$ di Student ottiene valori erroneamente più elevati; gli intervalli di confidenza tendono ad essere più stretti e l'area di rifiuto del test anomalamente più ampia).

Per individuare la caratteristica di autocorrelazione esistono diversi sistemi grafici:
\begin{itemize}[noitemsep]
  \item scatter plot della variabile risposta vs esplicative $y \sim x_j$ (da effettuare per ogni $x_j$);
  \item scatter plot dei residui vs variabili esplicative $\varepsilon \sim x_j$ (da effettuare per ogni $x_j$);
  \item scatter plot dei residui vs ritardati $\varepsilon \sim \varepsilon_{-1}$;
  \item correlogramma: è un grafico imn cui sono mostrate le correlazioni a diversi gradi; analizzando l'$acf$ (la funzione di autocorrelazione dei residui) e $pacf$ si determina il tipo di modello autoregressivo.
\end{itemize}

Il test di Durbin-Watson invece offre uno strumento analitico per verificare la presenza di autocorrelazione a diversi gradi.
La sua ipotesi nulla è la mancanza di autocorrelazione: $H_0: \varrho_{-i} = 0$; mentre l'ipotesi alternativa può essere verificata su entrambe le code della distribuzione (bidirezionale) $H_1: \varrho_{-i} \neq 0$ o su una coda sola (unidirezionale destra o sinistra) $H_1: \varrho{_{-i}} ^>_< 0$.
La statistica $DW$ per l'autocorrelazione dei residui è definita come:
\begin{equation*}
  DW = \frac{\sum{(\varepsilon_i - \varepsilon_{i-1})^2}}{\sum{ \varepsilon_i^2}} \in [0, 4]
\end{equation*}
si dimostra inoltre che sotto ipotesi di omoschedasticità di $\varepsilon$, $DW = 2(1 - \varrho_{-i})$.
Il valore tende a $2$ in caso di mancanza di autocorrelazione, a $0$ in caso di massima autocorrelazione negativa ($DW = 0 \Leftrightarrow \varrho_{-i} = -1$) e $4$ positiva ($DW = 4 \Leftrightarrow \varrho_{-i} = +1$).
Convenzionalmente i valori critici per definire se l'autocorrelazione è significativa o meno sono $1$ e $3$ (è considerata significativa se $DW < 1 \lor DW > 3$).

Nel caso di autocorrelazione, il teorema di Aitken stabilisce che nella classe degli stimatori lineari per il modello di regressione \textit{generalizzato} lo stimatore GLS è il più efficiente, ovvero è quello caratterizzato dalla minor varianza.

\section{Metodo di stima WLS, per soluzioni correlate, GLS }
\subsection*{Errori eteroschedastici e incorrelati: modello WLS}
Per errori eteroschedastici si intende quando la varianza dell’errore non è costante e il valore dipende dalle variabili esplicative ($\varepsilon \dep X$), violando quindi una delle ipotesi della regressione lineare classica.
Per tale motivo gli stimatori OLS non possono essere usati (in quanto non più efficienti); al contrario si possono utilizzare gli stimatori Weighted Least Squares (WLS) che permettono di stimare la varianza delle singole componenti erratiche $\varepsilon_i$ condizionatamente al vettore dei dati $x_i$.
Si esegue quindi una trasformazione della variabile risposta $y \rightarrow y^*$ e della matrice del disegno $X \rightarrow X^*$ per riportare la varianza degli errori ad una costante: si divide infatti ogni variabile per la radice di $h(i)$ (la varianza di $\varepsilon^*$, l'errore eteroschedastico).
La componente erratica dunque ha valore costante e il modello assume la forma:
% Si tratta di definire le seguenti nuove variabili che danno luogo al modello trasformato dividendo ogni variabile contenuta nel modello di partenza per la radice di $h(i)$ (corrispondente alla varianza di $\varepsilon$*, ovvero l'errore eteroschedastico). 
% Infatti si tratta di stimare i parametri del modello trasformato con il metodo OLS regredendo $y^*$ su $X^*b$ riportando così la varianza degli errori ad una costante ottenendo la forma: 
\begin{equation*}
y^* = X^*\beta + \varepsilon
\end{equation*}
permettendo una stima $b$ dei parametri $\beta$ tramite il metodo OLS.

\subsection*{Errori omoschedastici e correlati: modello GLS}
Nel caso invece ci si trovi davanti ad errori autocorrelati come accade in serie storiche e territoriali è ragionevole ipotizzare che esista correlazione fra errori in momenti successivi o territori vicini.
Si parla di autocorrelazione se al variare di $X$ c'è fluttuazione dei valori di $Y$ con lo stesso segno (autocorrelazione \textit{positiva}), o segno opposto (autocorrelazione \textit{negativa}), oltre un certo intervallo di confidenza.
Si possono ricavare stime per errori correlati in modo più semplice tramite una stima dei parametri in una equazione che tenga conto della struttura di autocorrelazione seriale (metodo proposto da Durbin).
Bisogna innanzitutto stimare il coefficiente di autocorrelazione di $i$-esimo ordine attraverso un modello avente come variabile risposta gli errori $\varepsilon_t$ e come esplicative quelle già considerate più l’errore ritardato di $i$ tempi $\varepsilon_{t-i}$ e procedere alla stima del coefficiente di correlazione $\varrho$.
Vale infatti:
\begin{equation*}
  \varepsilon_t = \varrho \varepsilon_{t-i} - \delta_t
\end{equation*}
dove $\delta$ è la componente erratica che segue le ipotesi classiche.
Si moltiplica dunque ogni elemento dell'equazione ritardata per $\varrho$.
% Una volta ottenuta la stima di $\varrho$ si procede a moltiplicare ogni elemento dell'equazione ritardata per $\varrho$ stesso: 
\begin{equation*}
  \varrho y_{t-i} = \varrho X_{t-i} \beta + \varrho \varepsilon_{t-i}
  % \varrho y_{t-1} = \varrho\beta_0 + \varrho\beta_1 x_{t-1} +\varrho\varepsilon_{t-1}^\#
\end{equation*}
Infine si procede a sottrarre l'equazione ritardata moltiplicata per $\varrho$ all'equazione nella forma normale $y_t - \varrho y_{t-i}$ ottenendo un modello OLS per i parametri trasformati: 
\begin{alignat*}{6}
  &y_t^\# &&= &&X_t^\# &&\beta + &&\varepsilon_t^\# \\
  (y_t &- \varrho y_{t-i}) &&= (X_t &&- \varrho X_{t-i}) &&\beta + (\varepsilon_t &&- \varrho \varepsilon_{t-i})
  % y_t^\# &= \beta_0^\# + \beta_1 x_t^\# + w_t \\
\end{alignat*} 
% dove
% \begin{equation*}
% \begin{matrix}
% y_t^\# = y_t - \rho y_{t-1}\\
% \beta_0^\# = \beta_0(1 -\rho)\\
% x_t^\# = x_t - \rho x_{t-1}\\
% w_t =\varepsilon_t^\# - \rho \varepsilon_{t-1}^\# 
% \end{matrix}
% \end{equation*}
Il modello rispetta tutte le proprietà classiche di correttezza, consistenza ed efficienza. 

In alternativa è possibile utilizzare un modello \textit{autoregressivo} che inserisce nell’equazione iniziale un errore ritardato che tenga conto dell’autocorrelazione di $i$-esimo ordine:
\begin{equation*}
  y = X \beta + AR_i + \varepsilon
  % y_i=b_0 +b_1 x_i +AR1_i  + \varepsilon_i. 
\end{equation*}
con 
\begin{align*}
  AR_i + \varepsilon &= v \\
  Corr(v_j,v_k) &= 0 \hspace{15pt} j \neq k
\end{align*}


\subsection*{Errori eteroschedastici e correlati: stimatore GLS}
Nel caso in cui gli errori non siano sferici in quanto eteroschedastici e correlati si utilizzano gli stimatori dei minimi quadrati generalizzati ($GLS$) interpretabili in modo analogo al modello classico in quanto stimatori $OLS$ basati su variabili trasformate per mezzo delle proprietà degli autovettori e autovalori ricavati dalla matrice dei residui $\Sigma_\varepsilon$.
Nello specifico si procede ad effettuare una \textit{decomposizione spettrale} della matrice degli errori:
\begin{equation*}
  \Sigma_\varepsilon = \sigma^2 V V^\prime
\end{equation*}
con
\begin{equation*}
V = \sigma (\sqrt{AL})A^\prime
\end{equation*}
dove $A$ è la \textit{matrice degli autovettori} e $L$ è la matrice diagonale degli autovalori di $\Sigma_\varepsilon$.
A questo punto premoltiplicando per $V^{-1}$ le componenti del modello si ottiene una nuova funzione con variabili trasformate e $\Sigma_{\varepsilon^\circ}$ omoschedastica ed incorrelata:
\begin{align*}
  V^{-1}y &= y^\circ \\
  % V^{-1}X\beta + V^{-1}\varepsilon^\circ = X^\circ\beta^\circ + \varepsilon \\
  V^{-1}X &= X^\circ \\
  y^\circ &= X^\circ \beta^\circ + \varepsilon
\end{align*}
Lo stimatore $b^\circ$ risulta godere delle proprietà di correttezza e consistenza; inoltre secondo il Teorema di Aitken è lo stimatore più efficiente per il modello \textit{generalizzato} (nonostante abbia una varianza maggiore rispetto al metodo OLS $\sigma^2 (X^{\circ \prime} X^\circ)^{-1} > \sigma^2 (X^\prime X)^{-1}$).
% \begin{enumerate}[noitemsep]
% \item Correttezza;
% \item Consistenza;
% \item Efficienza in quanto il teorema di Aitken stabilisce che nella classe degli stimatori lineari per il modello di regressione \textit{generalizzato} lo stimatore $GLS$ è caratterizzato dalla minima varianza, che risulta comunque maggiore di quella ottenuta attraverso il modello $OLS$ per i modelli lineari ma, condizionatamente ai modelli lineari generalizzati è il migliore. Infatti $\sigma^2 (X^{\circ '}X^{\circ})^{-1} >\sigma^2 (X'X)^{-1}$, e la differenza tra le due risulta essere una matrice semidefinita \textit{positiva};
% \item Lo stimatore assegna un peso maggiore alle osservazioni caratterizzate da una minore varianza da considerarsi più ``affidabili''.
% \end{enumerate}
Tuttavia la stima GLS necessita di assumere come nota la matrice di varianze e covarianze dei residui $\Sigma_\varepsilon$, o almeno poter calcolare una sua stima, a patto però che sia consistente al limite:
% Nel caso in cui questa non fosse nota allora è possibile ricorrere alla matrice campionaria $S_\varepsilon$ in modo che rispetti la condizione 
\begin{equation*}
  \lim_{n\to \infty} S_\varepsilon = \Sigma_\varepsilon
\end{equation*}
A questo punto si possono utilizzare gli stimatori FGLS (\textit{Feasible Generalized Least Squares}).
Spesso la soluzione di applicare i FGLS viene intrapresa anche in caso di semplice eteroschedasticità o semplice autocorrelazione, o sospette tali, poiché vige il principio di precauzione.

\section{Multicollinearità }
Se la matrice $(X^\prime X)$ non è invertibile o ha determinante prossimo allo 0, le stime non possono essere calcolate (coefficienti sotto identificati, poiché non si dispone di sufficiente informazione per stimarli) o non sono affidabili (coefficienti empiricamente sotto identificati).

Tale problema si verifica quando almeno una delle variabili è correlata linearmente alle altre: si ha quindi multicollinearità.
In questo caso la matrice $(X^\prime X)$ è detta singolare e l'inversa $(X^\prime X)^{-1}$ non è unica. 

Esistono due tipi di collinearità:
\begin{itemize}[noitemsep]
  \item \textbf{perfetta}: sussiste quando almeno una variabile esplicativa è una combinazione lineare perfetta delle altre: questa viola le proprietà del modello lineare classico; solitamente ciò si verifica per un errore nella definizione dei regressori o per la presenza di due variabili direttamente dipendenti una dall'altra (ad esempio \textit{titolo di studio} e \textit{anni di studio});
  \item \textbf{imperfetta}: sussiste in caso di forte correlazione tra i regressori e dunque il determinante della matrice dei coefficienti tende a $0$; ciò non rende impossibilità la stima dei coefficienti ma da origine a stime fortemente distorte e caratterizzate da un'alta varianza.
\end{itemize}

L'errore di stima provoca un aumento della varianza dello stimatore dei coefficienti $b$, da cui deriva una sovrastima delle dimensioni degli intervalli di confidenza (che risultano più ampi del dovuto) e una maggiore zona di accettazione nei test statistici di quanto sarebbe corretto.
Inoltre, l'aggiunta di una variabile fortemente correlata ad una già presente nel modello aggiunge poca informazione (sarebbe opportuno calcolare la correlazione \textit{spuria}).

Per individuare fenomeni di multicollinearità, è buona norma, in prima istanza, generare una \textit{matrice di correlazione} tra tutte le variabili così da identificare rapidamente possibili variabili collineari.
Auspicabilmente infatti è preferibile avere forte correlazione tra $y$ e i regressori $x_j$, e bassa correlazione tra questi ultimi.
Esistono inoltre metodi analitici:

\paragraph{Indice di tolleranza.} Misura il grado di interrelazione di una variabile indipendente rispetto alle altre.
Si calcola come $TOL = 1 - R^2 \in [0, 1]$ dove $R^2$ è il coefficiente di correlazione della regressione di una variabile esplicativa $x_j$ (usata come risposta) in funzione delle altre.
Valori alti indicano una bassa multicollinearità tra la singola variabile $x_j$ e le altre.

\paragraph{Varianza multifattoriale (o VIF).} È il reciproco della tolleranza $VIF = TOL^{-1}$.
Valori di tale indice variano tra $0$ e $\infty$, ma si considera significativo già se superiore a $20$ ($TOL = 0.05$) indicando uno stretto rapporto tra la variabile considerata e le altre del modello, ovvero un eccessivo grado di \textit{multicollinearità}.
Vanno considerate con attenzione anche quelle variabili con valori di VIF maggiori di $10$ ($TOL = 0.1$);

\paragraph{L'indice di condizione.} È dato dalla radice del rapporto tra l’autovalore massimo della matrice $(X^\prime X)$ e gli altri autovalori.
Quando risulta essere maggiore di $30$ si considera significativa la presenza di \textit{multicollinearità}.
Tale convinzione viene rafforzata se un autovalore con \textit{condition index} maggiore di $30$ contribuisce a spiegare elevate quote di varianza di due o più variabili.

\section{Linearità}
La relazione ipotizzata tra la nostra variabile dipendente $y$ e le singole variabili esplicative $x$ è di tipo: $y = f (x)$, con $f$ lineare.

L'approssimazione lineare non è sempre la migliore.
Per validare la presenza di ciascun regressore all’interno dei diversi modelli dobbiamo quindi verificare la linearità di tale relazione.
Dunque, la variabile risposta deve essere una combinazione lineare di variabili esplicative e di parametri lineari.

Se una relazione tra $y$ e $X$ non è lineare, allora l’effetto su $y$ ($\Delta y$)  di una variazione in $X$ ($\Delta X$) dipende puntualmente dal valore di $X$ poiché l’effetto marginale di $X$ non è costante. 

In questo caso, una regressione lineare è mal specificata: la forma funzionale è errata e lo stimatore dell’effetto su $y$ di $X$ non è corretto nemmeno sulla media. 
Può capitare ad esempio che l'indice $R^{2}$ sia elevato ma che non ci sia linearità perchè c'è sia una componente lineare sia una non lineare. 

Per verificare la presenza (o meno) di linearità è possibile ricorrere ad alcuni grafici: 
\begin{itemize}[noitemsep]
\item scatter plot della variabile risposta vs esplicative $y \sim x_j$ (da effettuare per ogni $x_j$);
\item scatter plot dei residui vs la variabile risposta $\varepsilon \sim y$ (non deve presentare andamenti sistematici);
  \item scatter plot dei residui vs valori previsti $\varepsilon \sim \hat{y}$ (l'andamento deve essere regolare).
% \item Scatter plot della  variabile risposta ($y_i$) in funzione di ogni esplicativa ($x_j$) presente nel modello;
% \item Scatter plot dei residui ($\varepsilon_i$) in funzione dei valori osservati ($y_i$) della variabile dipendente; non deve essere un andamento sistematico;
% \item Scatter plot dei residui ($\varepsilon_i$) in funzione dei valori previsti ($\hat{y_i}$); deve esserci un andamento regolare.
\end{itemize}
  
È da notare che la non linearità potrebbe dipendere anche solo da una o da alcune variabili esplicative e non necessariamente da tutte.
Quando è presente non linearità dei parametri, potrebbe esistere una trasformazione che li renda lineari (caso linearizzabile) oppure che questi siano espressi in una forma intrinsecamente non lineare.

Nel primo caso si procede innanzitutto alla linearizzazione del parametro (o della variabile) \textit{non lineare} con una trasformazione che lo renda \textit{lineare}, poi si procede alla stima OLS ed infine si applica la trasformazione inversa ricavando la stima del parametro originale. 
Nel caso invece di componenti intrinsecamente non lineari si procede allora alla stima attraverso gli stimatori NLS (minimi quadrati non lineari) che sfruttano algoritmi numerici nei software per affrontare il problema di minimizzazione non lineare.

Volendo utilizzare funzioni di variabili indipendenti non lineari in X possiamo riformulare una vasta famiglia di funzioni di regressione lineare come regressioni multiple.

Tra le funzioni non lineari le più utilizzate sono le polinomiali e le trasformazioni logaritmiche. 
%Queste ultime in particolare possono essere applicate alle variabili esplicative, a quella dipendente o ad entrambe le tipologie. %Superfluo?

Tra la trasformazioni logaritmiche esistono tre modelli principali: 
\begin{itemize}[noitemsep]
\item \textbf{Linear-log}, in cui ad un incremento percentuale della variabile indipendente corrisponde un incremento nominale %lineare?
 $\beta$ della variabile dipendente.
\item \textbf{Log-linear}, in cui ad un incremento nominale dell’esplicativa corrisponde un incremento percentuale $\beta$ della risposta.
\item \textbf{Log-log}, in cui entrambi gli incrementi sono percentuali.
\end{itemize}
Tuttavia la trasformazione di una variabile, eccettuando casi particolari in cui il dominio lo permette (la trasformata $log$-$log$ in un grafico quantità-prezzo indica l'\textit{elasticità}) , rende di difficile interpretazione il modello.

\section{Non normalità}

Quando gli errori $\varepsilon_i$ sono indipendenti e identicamente distribuiti come $N(0,\sigma^{2})$ si possono ricavare la distribuzione degli stimatori, i test statistici, gli intervalli di confidenza e le proprietà ottimali (inoltre stima di massima verosimiglianza $ML$ coincide con stima dei minimi quadrati $OLS$). 
Nel caso in cui gli errori non siano normali, se tuttavia i campioni sono sufficientemente larghi per il \textbf{teorema del limite centrale} la distribuzione degli errori tende \textit{asintoticamente} alla normalità. Se ciò non accade non è possibile applicare test e intervalli di confidenza perchè essi sono basati tutti sull’ipotesi di normalità degli errori.
%till here @Dario

Conseguenze della violazione della normalità:
\begin{enumerate}[noitemsep]
\item I parametri $\beta$ possono essere espressi come combinazione lineare degli errori, per cui se gli errori non sono normali anch’essi non sono più normali;
\item Non è più possibile ricavare test basati sulla normale standardizzata;
\item Non è più possibile ricavare intervalli di confidenza per i parametri basati sulla normale standardizzata;
\item Le stime OLS non coincidono con le stime ML ottenute attraverso il metodo della massima verosimiglianza, quindi gli stimatori OLS non sono più gli stimatori corretti a minima varianza \textit{fra tutti gli stimatori corretti} cioè non sono più \textbf{VUE}. Il fatto che le stime ML ed OLS non coincidano più rende meno affidabili le stime attraverso software statistici, che comunemente effettuano la stima attraverso il metodo della massima verosimiglianza. Nonostante gli stimatori OLS non siano più \textbf{VUE}, conservano le proprietà di correttezza, consistenza ed efficienza condizionatamente ai dati. Essendo i dati affetti da un bias sulla distribuzione dei residui $\varepsilon$ anche le stime OLS erediteranno tale bias ma, proprio in virtù di ciò, possono essere ancora considerati gli stimatori a minima varianza \textit{tra tutti gli stimatori lineari} e perciò sono considerati \textbf{BLUE}.
\end{enumerate}
Per individuare casi di non normalità è opportuno:
\begin{itemize}[noitemsep]
\item Osservare indici descrittivi;
\item Effettuare rappresentazioni grafiche;
\item Effettuare test non parametrici (ovvero realizzati con lo scopo di testare la distribuzione del parametro sotto osservazione).
\end{itemize}
Tra gli indici descrittivi possiamo, in prima istanza, osservare indicatori quali \textbf{moda}, \textbf{media} e \textbf{mediana}. Banalmente quando queste corrispondono possiamo affermare che la distribuzione dei residui $\varepsilon_i$ è normale. Questi indicatori sono anche visualizzabili in maniera diretta utilizzando un box-plot.

Tra la rappresentazioni grafiche utili rientrano:
\begin{itemize}[noitemsep]
\item Plot della distribuzione dei residui, per cui se la media risulta maggiore della mediana allora sarà possibile visualizzare una distribuzione caratterizzata da asimmetria \textit{positiva}(a destra), mentre in caso di media inferiore alla mediana sarà possibile visualizzare una distribuzione affetta da asimmetria \textit{negativa}(a sinistra).
\item Plot della distribuzione cumulata dei residui, che è possibile ispezionare alla ricerca di evidenti irregolarità.
\item P-P plot che mette a confronto la distribuzione cumulata dei residui (sulle ascisse) con la distribuzione cumulata della normale (sulle ordinate). Il risultato di ciò è che in caso di distribuzione normale allora i punti si distribuiranno in modo ordinato lungo la \textit{bisettrice}. 
\item Q-Q plot, molto simile al precedente, mette a confronto i quantili della distribuzione normale (sulle ascisse) con i residui $\varepsilon$ (sulle ordinate). Anche in questo caso la distribuzione dei punti lungo la \textit{bisettrice} indica il soddisfacimento dell'assunzione di normalità dei residui. La diversa forma assunta dai punti sulla bisettrice può inoltre indicare una distribuzione leptocurtica, platicurtica oppure asimmetrica (a destra o a sinistra a seconda della forma assunta).
\end{itemize}
Esistono, infine, alcuni test non parametrici che non si basano su ipotesi sulla distribuzione ma che sono appunto detti non parametrici poichè testano la distribuzione dei parametri. Per questo motivo sono molto utili per analizzare problemi di normalità dei residui.
\begin{enumerate}
\item \textbf{Test di Shapiro-Wilk}, che assume valori compresi tra 0 e 1 e gli estremi corrispondono rispettivamente al rifiuto e all’accettazione dell’ipotesi di normalità. Il test parte dell'ipotesi $H_0$ : $\varepsilon\sim N(0,\sigma^2)$. In ogni caso, il test $W$ essendo caratterizzato da una forte asimmetria  potrebbe comunque portare ad un rifiuto dell'ipotesi di normalità.
\begin{equation*}
W = \sum_{i}\frac{(\beta_i \varepsilon_i)^2}{ 			\varepsilon_i^2}
\end{equation*}
\item \textbf{Test di Kolmogorov Smirnov}, in cui $H_0$ : $\varepsilon\sim N(0,\sigma^2)$ e si basa sul calcolo della statistica test $D$ come la somma in valore assoluto della differenza tra le frequenze cumulate della distribuzione empirica da testare e quelle della normale, una volta definite delle classi di eguale ampiezza. $D$ viene poi messa a confronto con le apposite tavole (essendo una statistica tabulata) ed in caso di superamento del valore critico in base al livello di significativà scelto comporterà il rifiuto o l'accettazione di $H_0$;
\item \textbf{Skewness test} (test di asimmetria), ovvero un test direzionale basato sul fatto che la distribuzione della normale è simmetrica; si basa perciò su un indice di simmetria; rigettando $H_0$ si rigetta la normalità, non rigettandola, invece, si dice solo che la distribuzione è simmetrica, ma non per forza normale.
\begin{equation*}
S = \frac{(E[X-\mu]^3)^2}{(E[X-\mu]^2)^3}
\end{equation*}
Sinteticamente si tratta di mettere a rapporto il quadrato del momento terzo intorno alla media di $X$ con il cubo della varianza.
\newline
Quando l'ipotesi di normalità è rispettata allora $E(S) = 0$;
\item \textbf{Test della Kurtosis}, simile nella forma a quello per l'asimmetria
\begin{equation*}
K = \frac{E(X-\mu)^4}{(E[X-\mu]^2)^2}
\end{equation*}
Anche in questo caso sinteticamente si tratta di mettere a rapporto il momento quarto intorno alla media di $X$ con il quadrato della varianza.
\newline
Sotto l'ipotesi di normalità $E(K-3) = 0$, poichè la curtosi della normale è appunto uguale a 3.
\end{enumerate}
I problemi di non normalità possono essere risolti usando una \textit{trasformazione} della variabile dipendente $Y$. La trasformazione può migliorare la relazione lineare tra la variabile dipendente e le variabili indipendenti.

Tra le trasformazioni disponibili vi sono:

\begin{itemize}
\item \textit{log(Y)} quando $S_\varepsilon$ cresce con $y$ o quando la distribuzione dell'errore ha asimmetria \textit{positiva};
\item \textit{$Y^2$} quando $S_\varepsilon$ è proporzionale a $E(y)$ o quando la distribuzione dell'errore ha asimmetria \textit{negativa};
\item \textit{$\sqrt{Y}$} quando $S_\varepsilon$ è proporzionale a $E(y)$;
\item \textit{$Y^{-1}$} quando $S_\varepsilon$ cresce significativamente al crescere di $y$.
\end{itemize}

\section{Outlier}
I valori cosiddetti \textbf{outlier} possono essere distinti in:
\begin{enumerate}[noitemsep]
\item Valori anomali: valori che si discostano in modo rilevante dall’andamento generale.
\item Punti influenti: punti che influenzano in misura rilevante le stime.
\end{enumerate}
Non sempre un valore anomalo è anche influente; per contro esistono punti non anomali che influiscono in misura rilevante sul risultato.

Come identificare gli outlier:
\begin{itemize}[noitemsep]
\item Rappresentazioni grafiche per mezzo di box-plot e scatter-plot.
\item Indicatori
\end{itemize}
Tra questi \textbf{indicatori} è possibile distinguere tra:
\begin{enumerate}
\item \textbf{Leverage values}: Definita $H = X(X'X)^{-1}X'$, nota come matrice di proiezione, gli elementi $h_{ii}$ sulla diagonale, chiamati leverage, possono essere usati per verificare l'impatto dell'osservazione $i$-esima sulla capacità del modello di predire tutti i casi.

Si dimostra che il valor medio del leverage è:
%just per coerenza
\begin{equation*} 
\frac{(k-1)}{n}
\end{equation*}
con $k = n^\circ$ variabili esplicative ed $n = n^\circ$ osservazioni.

Può dunque essere considerato 
\begin{equation*}
h_{ii} > \frac{2(k-1)}{n}
\end{equation*}
come valore soglia per individuare osservazioni potenzialmente anomale con un'eccessiva influenza sulla stima complessiva di tutte le osservazioni 
\item \textbf{Residui standardizzati}: Assumendo per i residui $\varepsilon$
\begin{equation*}
\varepsilon = (I-H)y
\end{equation*}
allora è possibile scrivere la varianza esplicitata come
\begin{equation*}
Var(\varepsilon_i) = (1-h_{ii})\sigma^2
\end{equation*}
Come conseguenza di ciò i residui \textit{standardizzati} sono definiti come
\begin{equation*}
\varepsilon_i^* = \frac{\varepsilon_i}{\sigma \sqrt{(1-h_{ii})}}
\end{equation*}
In un campione distribuito normalmente il $95\%$ dei valori dei residui standradizzati $\varepsilon_i^*$ dovrebbe assumere valori compresi tra $-2$ e $+2$ mentre il $99\%$ dovrebbe assumere valori compresi tra $-2.5$ e $+2.5$; nel caso in cui il valore del residuo standardizzato fosse maggiore di $3$ probabilmente l’osservazione corrispondente sarà un outlier.
\item \textbf{Residui studentizzati}: è la versione dei residui standardizzati ma relativamente al campione.
Di conseguenza le forme analitiche saranno le medesime facendo però riferimento non alla varianza $\sigma^2$ ma alla varianza campionaria $s^2$.
\begin{equation*}
\varepsilon_i^* = \frac{\varepsilon_i}{s_{\varepsilon i} \sqrt{(1-h_{ii})}}
\end{equation*}
Sono utilizzati per verificare la presenza di osservazioni anomale in campioni di non elevata numerosità. La versione dei residui studentizzati cosiddetta \textit{jacknife} è ricavata calcolando il rapporto dei residui sulla deviazione standard dei residui ottenuta eliminando dal dataset l'$i$-esima osservazione, così per ogni residuo studentizzato.
\item \textbf{Covrati}: indica la variazione nel determinante della matrice delle covarianze delle stime eliminando la $i$-esima osservazione. Eliminando infatti il valore $i$-esimo provoco una variazione nel determinante che vado a quantificare.
\begin{equation*}
\mathrm{COVRATIO} = \frac{(det(\sigma_i X_i'X_i^{-1}))}{det(\sigma^2(X_i'X_i)^{-1}))}
\end{equation*}
Il valore di soglia è determinato da
\begin{equation*}
1\pm 3 \  \sqrt{\biggl(\frac{(k+1)}{n}\biggr)}
\end{equation*}
\item \textbf{Dfitts}: misura l’influenza dell'$i$-esima osservazione sulla stima dei coefficienti di regressione e sulla loro varianza, eliminandola dal dataset. Osservazioni con valori elevati di Dfitts sono associati a punti influenti. Con $\hat{y} - \hat{y}_{(i)}$ verifico infatti l'impatto che la rimozione dell'$i$-esima osservazione ha sul valore finale dell'output del modello.
\begin{equation*}
\mathrm{DFITTS} = \frac{\hat{y} - \hat{y}_{(i)}}{S_{e(i)}\sqrt{h_{ii}}}
\end{equation*}
con valore soglia
\begin{equation*}
\pm 2 \ \sqrt{\biggl(\frac{(k+1)}{n}\biggr)}
\end{equation*}
\item \textbf{Dfbetas}: misura l’influenza dell'$i$-esima osservazione sulle stime di ogni coefficiente di regressione separatamente, eliminandola dal dataset. Ancora una volta valori elevati indicano che l'osservazione influisce molto sulla stima dei parametri. Per questo indice infatti un valore è ritenuto anomalo non se cambia il valore previsto ma se cambia anche solo uno dei coefficienti, per sua natura è quindi un indice molto più stringente rispetto al precedente.
\begin{equation*}
\mathrm{DFBETAS} = \beta - \beta_{(i)} = X_{(i)}(X'X)^{-1}\frac{\varepsilon_i}{1-h_{ii}}
\end{equation*}
con valore soglia $2$ oppure $2\sqrt{n}$ in caso si voglia tenere conto della numerosità delle osservazioni.
\item \textbf{Distanza di Cook}: misura l’influenza dell'$i$-esima osservazione sulla stima dei coefficienti di regressione \textit{nel loro complesso}, in termini di capacità del modello di predire tutti i casi quando la singola osservazione viene rimossa dal dataset, per questo motivo è molto simile al Dfitts. Valori superiori a 1 ( o eventualmente a $4/n$, essendo $n$ il numero di osservazioni) indicano che il punto è influente.
\begin{equation*}
D_i = \frac{(\beta - \beta_{(i)})(X'X)(\beta - \beta_{(i)})}{k\sigma_{(i)}^2}
\end{equation*}
\end{enumerate}

\section{Modello lineare classico multivariato}

Consideriamo l’estensione multivariata (con più di una variabile dipendente) della regressione lineare multipla (con più di un regressore) che modella la relazione fra un insieme di $r$ variabili esplicative $z_1$,..,$z_r$, ed $m$ variabili dipendenti $y_1$,..,$y_m$. Ognuna delle $m$ variabili dipendenti è legata a una particolare regressione multipla. 

Per l'$i$-esimo individuo abbiamo:
\begin{align*}
y_i = [y_{i1},...,y_{ij},...,y_{im}]\\
z_i = [1,z_{i1},...,z_{ik},...,z_{ir}]\\
\varepsilon_i = [\varepsilon_{i1},...,\varepsilon_{ij},...,\varepsilon_{im}]
\end{align*}
Mentre la matrice dei parametri $\beta$ (m, r+1) per le m equazioni è:
\[
\beta =
\begin{bmatrix}
\beta_{10} & \dots & \beta_{1k} & \dots & \beta_{1r} \\
\vdots& \ddots & \vdots& \vdots & \vdots \\
\beta_{j0} & \dots & \beta_{jk} & \dots & \beta_{jr} \\
\vdots& \vdots& \vdots &\ddots & \vdots \\
\beta_{m0} & \dots & \beta_{mk} & \dots & \beta_{mr} 
\end{bmatrix}
\]
In sintesi ogni \textit{riga} della matrice dei parametri $\beta$ si riferisce ad una variabile risposta $y_{1,...,m}$ mentre ogni colonna si riferisce ad una variabile esplicativa $z_{1,...,r}$. 
\newline
%Ciò implica che anche riferendosi ad una singola $y_i$, comunque la stima avviene nel complesso, con il prodotto tra la matrice delle variabili esplicative $Z$ e appunto $\beta$ che comprende al suo interno però \textbf{tutti} i parametri stimati riguardanti l'insieme delle variabili dipendenti $y_{1,...,m}$, rendendo di fatto i parametri che stimano le diverse variabili dipendenti $y$ influenzati gli dagli altri. In questo particolare aspetto risiede infatti la differenza tra un modello multivariato su $m$ variabili dipendenti $y$ ed $m$ modelli univariati, ognuno con una specifica $y$ come variabile dipendente.%
Nel suo complesso perciò il modello multivariato appare come 
\[ Y_{(m,n)} = B_{(m,r+1)}Z_{(r+1,m)} + E_{(m,n)} \quad \text{ ovvero}\]
\[ \forall j \in [1,m] \quad y_j= \beta_{j0} + \beta_{j1}z_1 + \dots + \beta_{jr}z_r + \varepsilon_j \]
Con il contenuto della matrice delle variabili dipendenti interpretabile come:
\begin{itemize}[noitemsep]
\item Ogni colonna rappresenta un individuo con i valori assunti dalle $y_m$ variabili dipendenti per quell'individuo.
\item Ogni riga rappresenta il valore assunto dalla singola variabile dipendente $y_i$ su tutti gli individui.
\end{itemize}
Le ipotesi del modello sono analoghe a quelle formulate per il modello univariato ma, essendo applicate su più variabili dipendenti risultano molto più stringenti:
\begin{enumerate}[noitemsep]
\item Parametri lineari;
\item Valori attesi degli errori casuali sono nulli $E(\varepsilon_{ij}) = 0$;
\item Gli errori casuali all’interno gli ogni equazione e \textit{anche tra diverse equazioni} sono omoschedastici e incorrelati.  La matrice di varianze e covarianze dei residui assume infatti la forma
\[ \Sigma_{E} = 
\begin{bmatrix}
\sigma^2 I_n & 0 & \dots & \dots & 0 \\
0 & \sigma^2 I_n & \dots & \dots &  0 \\
\dots&\dots&\dots&\dots&\dots \\
\dots&\dots&\dots&\dots&\dots \\
0 & 0 & \dots & \dots & \sigma^2 I_n
\end{bmatrix}
\]
Con dimensione $(nm,nm)$ poichè ogni matrice $\Sigma_\varepsilon$ relativa ad ogni singola equazione è di dimensione $(n,n)$ ed essendo $m$ il numero di variabili dipendenti $y$ otteniamo appunto una matrice di varianze e covarianze di questa dimensionalità. Mentre gli elementi diagonali di questa matrice rappresentano gli errori relativi alla medesima equazione, le matrici $0$ che non si trovano sulla diagonale, racchiudono le correlazioni fra gli errori relativi ad equazioni diverse. Per le matrici $0$ infatti abbiamo sulla diagonale la correlazione di \textit{ogni individuo} con sè stesso relativamente alle diverse variabili dipendenti $y$ (ovvero le scelte dell'$i$-esimo individuo riguardo una determinata $y_i$ non influenzerebbero le scelte dello stesso individuo riguardo un altra $y_j$, ipotesi molto forte) mentre per gli elementi non diagonali abbiamo la correlazione di ogni individuo \textit{con un altro} (questa ipotesi molto meno forte rispetto alla precedente) ;
\item Le  variabili esplicative $Z$ sono non stocastiche: per ogni osservazione, il valore delle $Z$ è una costante mentre il corrispondente valore di ogni $y$ è una variabile casuale influenzata dagli errori casuali;
\item Le $Z$ variabili esplicative sono non collineari con rango ($z=r+1$), contrariamente la matrice $Z'Z$ non sarebbe invertibile e non sarebbe calcolabile lo stimatore dei minimi quadrati;
\item La numerosità della popolazione $n$ è maggiore del numero degli $r$ parametri stimati più l'intercetta ($n > r+1$) per la stessa ragione, perciò per ogni equazione le stime dei minimi quadrati di $\hat{\beta}$ sono trovate in modo analogo al caso univariato:
\begin{equation*}
\hat{\beta} = y_j Z'(Z'Z)^{-1}
\end{equation*}
Di conseguenza \textbf{nel modello multivariato classico calcolare le soluzioni per ogni variabile dipendente $y$ singolarmente oppure tutte insieme, dal punto di vista descrittivo, è identico}.
\item Gli errori $E$ si distribuiscono come una normale multivariata:
\begin{equation*}
E \sim N(0,s^2 I_{nm})
\end{equation*}
con $0$ vettore delle medie  e $s^2 I_{nm}$ matrice di varianze e covarianze della variabile casuale multivariata $E$. Permane la condizione di ortogonalità poichè i residui sono incorrelati sia con le variabili esplicative $Z$ che con i valori predetti della variabile dipendente $\hat{Y}$.
\end{enumerate}
Inoltre poichè
\begin{equation*}
Y = \hat{Y} + \hat{E}
\end{equation*}
abbiamo che
\begin{equation*}
(\Sigma_Y = YY') = (\hat{H}=\hat{B}ZZ'\hat{B}')+(\hat{\Sigma}_E=\hat{E}\hat{E}')
\end{equation*}
Con $\Sigma_Y$ matrice di varianze e covariante di $Y$, $\hat{H}$ matrice di varianze e covariante \textit{spiegate} e $\Sigma_E$ matrice di varianze e covariante \textit{residue}.

La grossa differenza tra soluzione univariata e multivariata, però, sta nelle \textit{covarianze}, poichè a differenza della soluzione univariata, varianze spiegate e residue non sono scalari ma, appunto, matrici; occorre quindi tenere conto delle correlazioni tra le soluzioni.

Nel caso multivariato classico infatti le parti diagonali di $ \Sigma_E$ e $\hat{H}$ sono \textit{identiche}.
\begin{equation*}
(\Sigma_Y = YY') = (\hat{H}=\hat{B}ZZ'\hat{B}')+\sigma^2 I_{nm}
\end{equation*}
L’$R^{2}$ in quest'ottica è una media pesata degli $R^{2}$ delle singole equazioni (sempre tenendo conto della numerosità dei casi che per tipo di rilevazione e missing value può non essere uguale nelle diverse equazioni).

\begin{remark}[]\ \\
\begin{itemize}
\item La dipendenza della variabile dipendente $y_j$ da $Z$ \textbf{non influenza} la dipendenza delle altre variabili $y_m$.
\item Abbiamo le \textbf{stesse} variabili esplicative in tutte le equazioni del sistema.
\item La correlazione simultanea tra i disturbi è \textbf{costante} nel tempo.
\end{itemize}
\end{remark}
\section{Inferenza nella Regressione Multivariata}
Gli stimatori OLS sono corretti ed efficienti, poichè il teorema di Gauss-Markov vale anche per il caso multivariato. Infatti nell'ambito degli stimatori lineari e corretti del vettore dei parametri, lo stimatore $\beta$ dei minimi quadrati è quello a \textit{varianza minore}. Inoltre per il modello di regressione multivariata con rango pieno con errori $E$ normalmente distribuiti anche le $m$ variabili dipendenti $Y$ sono distribuite secondo una normale multivariata $$Y \sim N(BZ, \Sigma_Y)$$ come anche i parametri stimati $\beta$ $$\beta \sim N(B,\hat{H})$$ con $\hat{H}$ matrice di varianze e covarianze spiegate della popolazione, positiva definita ed efficiente, e che si dimostra essere distribuita in modo indipendente da $E$ matrice degli errori.
\'{E} però il caso di notare che sia $\Sigma_Y$ che $\hat{H}$ sono entrambi matrici di varianza e covarianza \textit{non diagonali} e, di conseguenza, sono influenzate dalle correlazioni.

Si definisce invece \textit{varianza generalizzata} di $\hat{H}$ il suo determinante. Decidiamo di utilizzare la varianza generalizzata di $\hat{H}$ perchè ci è impossibile usare sia $t$ che $F$ in quanto misure univariate. A proposito di quanto detto riguardo la non diagonalità di $\hat{H}$, la sua varianza generalizzata è proprio una misura di variabilità che considera la correlazione tra le variabili. 

La varianza generalizzata si dimostra infatti uguale a $0$ in caso di presenza di:
\begin{itemize}[noitemsep]
\item variabile costante nelle unità statistiche;
\item variabile perfettamente correlata con un'altra;
\item variabile combinazione lineare di altre variabili.
\end{itemize}
Analogamente si definisce \textit{varianza generalizzata} di $\Sigma_E$ il determinante della matrice di varianza-covarianza residua. 
\newline
\newline
Considerando che $\hat{H}$ si distribuisce come una variabile casuale di \textbf{Wishart} con $r$ \textit{gradi di libertà} e $\Sigma_E$ sempre come una Wishart con $(r-n)$ \textit{gradi di libertà} e considerando la Wishart una generalizzazione multivariata di $F$ possiamo quindi definire come \textbf{test del rapporto di verosimiglianza Lambda di Wilks}: \begin{equation*}
\wedge= \frac{|\Sigma_E|}{|\Sigma_E + \hat{H}|} 
\end{equation*}
Che si distribuisce \textit{asintoticamente} come una $\chi^2$ con $mr$ \textit{gradi di libertà}. \\
Sempre da $\wedge$, inoltre, si ricava una distribuzione asintotica di $F$
\begin{equation*}
F = \frac{(1-\wedge)}{\wedge}
\end{equation*}
che nel caso sia rispettata l'ipotesì di normalità dei residui
\begin{equation*}
E \sim N(0,\sigma^2 I_{nm})
\end{equation*}
permette di costruire \textit{test multivariati} per i parametri  del modello analoghi a quelli costruiti utilizzando $F$ nel caso univariato.\\
Il test del rapporto di verosimiglianza Lambda di Wilks assume come ipotesi nulla: 
\begin{equation*}
H_0:\hat{B} = 0 
\end{equation*} 
per cui nel caso $H_0$ si rivelasse vera allora $\wedge$ tenderebbe ad 1 per la struttura stessa di $F$. \\
Se infatti $H_0$ è vera il numeratore e il denominatore di $\wedge$ tenderanno a coincidere poichè $\hat{H}$ tenderà a $0$. Perciò la regione di accettazione di $H_0$ (nullità dei parametri $\hat{B}$) è per valori di $\wedge$ vicini all’$1$. Tenendo quindi conto di queste circostanze e per la struttura di $F$, per $\hat{H}$ che tende a $0$ anche $F$ tenderà a $0$, cadendo così nella \textit{regione di accettazione} del test.

La regione di rifiuto di $H_0$ è per valori di $\wedge$ più piccoli di $1$, in cui il numeratore è più piccolo del denominatore per la presenza di $\hat{H}$.
Data la struttura della $F$, al crescere di $\wedge$ decresce il numeratore e cresce il denominatore. In sintesi, quindi, se $H_0$ falsa allora $\hat{H}$ diventa più grande ed $F$ tende ad infinito cadendo nella \textit{regione di rifiuto} del test.
\newline
\newline
Perciò per il test basato su $F$ asintotica:
regione di accettazione di $H_0$ è per $p-value$ superiori ad $1-\alpha$;
regione di rifiuto di $H_0$ per $p-value$ inferiori ad $1-\alpha$;
analogamente si costruiscono intervalli di confidenza per i parametri e per i valori predetti delle $Y$.

Esistono inoltre altri test che possiedono la stessa distribuzione, impalcature ed $H_0$ della Lambda di Wilks:
\begin{itemize}[noitemsep]
\item \textbf{Traccia di Lawney-Hotelling}
\begin{equation*}
LH = \frac{|\hat{H|}}{|\Sigma_E|}
\end{equation*}
\item \textbf{Traccia di Pillai}
\begin{equation*}
P = \frac{|\hat{H}|}{|\hat{H}+\Sigma_E|}
\end{equation*}
\item \textbf{Massimo autovalore di Roy}
\begin{equation*}
\mathrm{Max \ autovalore\  di \ } \frac{|\hat{H}|}{|\hat{H}+\Sigma_E|}
\end{equation*}
\end{itemize}
In modo analogo ad $F$, si possono costruire altri test con $H_0$ particolari:
\begin{itemize}[noitemsep]
\item Test sulla non significatività di un gruppo di variabili esplicative rispetto a tutte le variabili dipendenti.
\begin{equation*}
H_0 : \hat{B} = 0
\end{equation*}
\item Test sull’uguaglianza dei parametri relativi a diversi gruppi di variabili esplicative nelle singole equazioni.
\begin{equation*}
H_0 : B_{kj} = B_{gj} 
\end{equation*}
\item Test sull'uguaglianza dei parametri relativi alle stesse variabili in coppie di diverse equazioni.
\begin{equation*}
H_0 : B_{cA} = B_{vA}
\end{equation*}
\end{itemize}


\section{Modello lineare generalizzato}
Il modello lineare multivariato generalizzato supera le ipotesi, molto stringenti, del modello lineare multivariato classico.
Quando cambiano le ipotesi sugli errori si ha il modello lineare generalizzato: $$Y = BZ+E$$ in cui la matrice di covarianza degli errori non è più necessariamente diagonale e gli errori potrebbero essere eteroschedastici. 

Nell’ipotesi \emph{classica} infatti abbiamo che:

\begin{enumerate}[noitemsep]
\item Gli errori sono \textbf{omoschedastici} all’interno delle stesse equazioni: per ogni individuo rispetto alla medesima variabile dipendente la parte spiegata è uguale;
\item Gli errori sono \textbf{omoschedastici} tra equazioni diverse: per ogni individuo rispetto alle diverse variabili dipendenti la parte spiegata è uguale;
\item Gli errori sono \textbf{incorrelati} all’interno delle stesse equazioni: il comportamento di ogni individuo rispetto alla medesima variabile dipendente non è legato a quello degli altri individui;
\item Gli errori sono \textbf{incorrelati} fra equazioni diverse: il comportamento di ogni individuo rispetto a diverse variabili dipendenti non è legato al proprio e a quello degli altri individui.
\end{enumerate}
Infatti ipotizzando 3 variabili dipendenti:
\begin{itemize}[noitemsep]
\item Spesa viaggi
\item Spesa partite
\item Spesa concerti
\end{itemize}
La parte di variabilità non spiegata dalle variabili esplicative ($\Sigma_{E}$) è identica per tutti gli individui, per ognuna delle 3  variabili dipendenti (omoschedasticità). Inoltre la stessa $\Sigma_E$ per l’individuo \textit{i}-esimo non è influenzata dall’individuo \textit{k}-esimo per ogni variabile dipendente. La parte non spiegata dalle variabili esplicative dell’individuo \textit{i}-esimo rispetto alla spesa viaggi (ad esempio) non è influenzata dalla parte non spiegata dalle variabili esplicative per lo stesso individuo \textit{i} rispetto alla spesa partite (incorrelazione).


Nell’ipotesi \emph{intermedia}, invece:
\begin{enumerate}[noitemsep]

\item Gli errori sono \textbf{omoschedastici} all’interno delle stesse equazioni: per ogni individuo rispetto alla medesima variabile dipendente la parte spiegata è uguale;
\item Gli errori sono \textbf{eteroschedastici} tra equazioni diverse: per ogni individuo rispetto alle diverse variabili dipendenti la parte spiegata è diversa;
\item Gli errori sono \textbf{incorrelati} all’interno delle stesse equazioni: il comportamento di ogni individuo rispetto alla medesima variabile dipendente non è legato a quello degli altri individui;
\item Gli errori sono \textbf{correlati} fra equazioni diverse: il comportamento di ogni individuo rispetto a diverse variabili dipendenti è legato al proprio e a quello degli altri individui.
\end{enumerate}
Infatti facendo riferimento all'esempio precedente potremmo dire che:
\begin{itemize}[noitemsep]
\item La parte di variabilità non spiegata dalle variabili esplicative è identica per tutti gli individui per spesa viaggi ad esempio (omoschedasticità nella stessa equazione);
\item La parte di variabilità non spiegata dalle variabili esplicative per tutti gli individui è diversa  per spesa viaggi, partite, concerti (eteroschedasticità fra diverse equazioni);
\item La parte non spiegata dalle variabili esplicative per l’individuo \textit{i} non è influenzata da quella dell'individuo \textit{k} per ogni variabile dipendente e per le varie voci di spesa (incorrelazione fra individui diversi);
\item La parte non spiegata dalle variabili esplicative dell’individuo \textit{i} rispetto alla spesa  è influenzata dalla parte non spiegata dalle variabili esplicative per lo stesso individuo \textit{i} rispetto alla spesa partite ad esempio (correlazione per medesimo individuo).
\end{itemize}
Nell’ipotesi \emph{estrema}:
\begin{enumerate}[noitemsep]

\item Gli errori sono \textbf{eteroschedastici} all’interno delle stesse equazioni: per ogni individuo rispetto alla medesima variabile dipendente la parte spiegata è diversa;
\item Gli errori sono \textbf{eteroschedastici} tra equazioni diverse: per ogni individuo rispetto alle diverse variabili dipendenti la parte spiegata è diversa;
\item Gli errori sono \textbf{correlati} all’interno delle stesse equazioni: il comportamento di ogni individuo rispetto alla medesima variabile dipendente è legato a quello degli altri individui;
\item Gli errori sono \textbf{correlati} fra equazioni diverse: il comportamento di ogni individuo rispetto a diverse variabili dipendenti è legato al proprio e a quello degli altri individui.
\end{enumerate}
Continuando l'esempio precedente avremo quindi tutto l'opposto dell'ipotesi classica:
\begin{itemize}[noitemsep]
\item La parte di variabilità non spiegata dalle variabili esplicative è diversa per tutti gli individui per spesa viaggi ad esempio (eteroschedasticità nella stessa equazione);
\item La parte di variabilità non spiegata dalle variabili esplicative per tutti gli individui è diversa  per spesa viaggi, partite, concerti (eteroschedasticità fra diverse equazioni);
\item La parte non spiegata dalle variabili esplicative per l'individuo \textit{i} è influenzata dall’individuo \textit{k} per ogni variabile dipendente come ad esempio fra spesa viaggi e spesa partite (correlazione fra individui diversi);
\item La parte non spiegata dalle variabili esplicative dell’individuo \textit{i} rispetto alla spesa  è influenzata dalla parte non spiegata dalle variabili esplicative per lo stesso individuo \textit{i} rispetto ad esempio alla spesa partite (correlazione per medesimo individuo)
\end{itemize}
Quindi occorre usare non le singoli sottomatrici di correlazione degli errori $\Sigma_{E(i)}$, ma la matrice $\Sigma_E$ relativa all’intero modello.

\section{Modello SURE}

Nel modello \textbf{S}eemengly \textbf{U}ncorrelated \textbf{R}egression \textbf{E}quation, anche detto \textbf{SURE}, si segue un approccio più realistico: degli $r$ regressori si usano solo i regressori effettivamente legati alle diverse variabili dipendenti, che potrebbero anche essere tutti, come nel caso classico del modello lineare multivariato, ma che in caso di non significatività potrebbero portare ad una differenziazione delle diverse equazioni. Inoltre questo modello permette di risolvere anche il problema di una numerosità diversa delle osservazioni tra le diverse equazioni.
\newline
In altre parole nel modello SURE abbiamo regressori diversi per ogni equazione all’interno dell’insieme complessivo dei regressori per l’insieme delle equazioni del modello. 
\newline
Quindi la somma di tutti i regressori nelle diverse equazioni è uguale a
\begin{equation*}
\sum_{j=1}^n r_j
\end{equation*}
Data $n_j$ come la numerosità delle osservazioni per l’equazione $j$-esima allora il complesso delle numerosità è dato da 
\begin{equation*}
\sum_{j=1}^m n_j
\end{equation*}
La soluzione dei minimi quadrati per la stima dei coefficienti sembra simile a quella dei minimi quadrati generalizzati ma solo in apparenza: 
\begin{itemize}
\item Il modello è caratterizzato dalla presenza delle variabili esplicative $Z_A$, $Z_B$, $Z_C$ diverse da equazione ed equazione.
\item Gli errori sono:
	\begin{itemize}[noitemsep]
	\item omoschedastici e incorrelati nella stessa equazione;
	\item eteroschedastici fra diverse equazioni;
	\item correlati per lo stesso individuo e incorrelati tra individui diversi fra diverse equazioni.
	\end{itemize}
\end{itemize}
Considerando $B^*$ come stimatore dei coefficienti del modello OLS e $\hat{B}^*$ come lo stimatore del modello SURE, possiamo affermare che i due risultano identici ma per la costruzione del modello SURE $\hat{B}^*$ possiede dei parametri uguali a zero proprio per permettere di ottenere equazioni differenziate nel numero delle variabili esplicative. La possibilità di differenziare il numero di covariate risiede infatti nel porre uguale a zero il valore della variabile non presente nell'equazione $x$ così che questa, moltiplicata per il rispettivo coefficiente $B$ generi un influenza nulla sulla stima di $Y$. 
\newline
\newline
\textbf{NB}: Quando si omettono delle variabili, essendo i coefficienti di regressione parziali (influenzati, cioè, dalle altre variabili esplicative poichè stimati sulla totalità di esse) cambiano tutti i coefficienti di regressione in base all'equazione di riferimento. Infatti ipotizzando di avere una variabile $X_j$ presente in 2 equazioni distinte il coefficiente $\beta$ corrispondente sarà diverso proprio in virtù del fatto che le stime sono influenzate le une dalle altre.

\section{Il problema dei dati gerarchici e uso di Regressione multilevel}

I modelli statistici, di solito, si basano sull'assunzione di indipendenza delle osservazioni, ottenuta per mezzo di un \textit{campionamento casuale semplice} da popolazione infinita o finita con reinserimento. In questo caso si dice che le osservazioni si distribuiscono come una variabile casuale e che sono \textbf{IID}, ovvero \textbf{I}denticamente ed \textbf{I}ndipendentemente \textbf{D}istribuite.
 
In molti casi, però i dati risultano essere raggruppati in cluster ovvero presentano una struttura gerarchica come ad esempio:
\begin{itemize}
\item  Ospedale - Pazienti;
\item  Classi - Studenti;
\item Imprese - Impiegati.
\end{itemize}
 In tali casi il campionamento casuale semplice non risulta efficiente, ma appare preferibile effettuare un \textit{campionamento a più stadi} perché si desidera analizzare le relazioni tra le variabili che possono essere misurate a livelli di raggruppamento diversi (livelli gerarchici della struttura dei dati). Questo tipo di campionamento implica infatti \textbf{dipendenza} tra le osservazioni appartenenti allo stesso gruppo. Ad esempio gli studenti appartenenti alla stessa scuola condividono stesso ambiente, stessi insegnanti, stesso quartiere di provenienza oltre a scambi e comunicazioni tra essi.

Quando i dati possiedono una struttura gerarchica significa che possono essere scomposti in dati \textit{dell'unità} e dati \textit{del gruppo}. La dipendenza tra le unità di primo livello (micro) appartenenti alla stessa unità di secondo livello (macro) è cruciale per l’analisi.
\newline
Cosa succede se si ignora la struttura gerarchica dei dati?
\begin{enumerate}[noitemsep]
\item Potrei \textbf{aggregare} i dati micro (le unità) a livello macro (le sovrastrutture), ad esempio le condizioni di lavoro degli impiegati di un'azienda non possono essere attribuite ai singoli impiegati, così facendo si andrebbe incontro a quella che è definita \textbf{fallacia ecologica}: se vi è correlazione tra variabili a livello \textit{macro} non può essere usata per fare asserzioni a livello \textit{micro}.
\item Potrei \textbf{disaggregare}, ovvero utilizzare i dati \textit{macro} a livello \textit{micro} ignorando la variabilità tra i gruppi (ad esempio, le caratteristiche degli studenti non possono dire nulla sulla scuola se non è messa in luce esplicitamente la loro appartenenza all'una o all'altra scuola), andando incontro a quella che è definita \textbf{fallacia atomistica}: se vi è correlazione tra variabili a livello \textit{micro} non può essere usata per fare asserzioni a livello \textit{macro}.
\end{enumerate}
Ipotizzando una \textbf{regressione Multilevel}, in cui la regressione di $Y$ su $X$ è la funzione lineare di $x$ che meglio spiega $y$ ed ipotizzando che i dati abbiano struttura ad un livello, nulla cambia rispetto al Modello lineare classico univariato, singolare o multiplo, infatti assume forma:
\begin{equation*}
y = \beta_0 + \beta_1 x + r
\end{equation*}
Ipotizzando invece una struttura a più livelli con:
\begin{itemize}
\item $j$ identificativo del gruppo;
\item $i$ identificativo dell'unità entro il gruppo;
\item $x_{ij}$ ed $y_{ij}$ osservazioni di $X$ e $Y$ sulle unità micro del gruppo $j$;
\item $\bar{x}_{.j}$ e $\bar{y}_{.j}$ medie di gruppo $j$ per $X$ e $Y$.
\end{itemize}
ad esempio il soggetto 1 del primo gruppo sarà diverso dal soggetto 1 del secondo gruppo. \\
In questo contesto la variabile dipendente $Y$, quindi, ha sia un aspetto individuale sia uno di gruppo; la variabile $X$ pur essendo misurata a livello individuale contiene anche una quota di variabilità imputabile al gruppo, infatti la media di $X$ in un gruppo può essere diversa dalla media di $X$ in un altro gruppo poiché la composizione della $X$ nei gruppi può essere diversa.
\newline
Le regressioni a livello \textit{macro} considerano i dati aggregati dati dalla media di $X$ ed $Y$ $$\bar{y}_{.j} = \beta_0 + \beta_1\bar{x}_{.j}+ r_{.j}$$ e sono quindi diverse dalle regressioni a livello \textit{micro} tra $X$ ed $Y$ $$y_{ij} + \bar{y}_{.j} = \alpha_1 (x_{ij} - \bar{x}_{.j})+r$$
L’analisi delle relazioni entro i gruppi può portare a risultati molto diversi da quelli ottenuti considerando le relazioni tra i gruppi. In altri termini la struttura dei dati ed il loro raggruppamento può avere effetto anche in un altro modo: facendo variare i coefficienti della regressione da gruppo a gruppo
\begin{equation*}
Y_{ij} = \beta_{0j} + \beta_{ij} x_{ij} + r_{ij}
\end{equation*}
con diverse intercette $\beta_{0j}$ e coefficienti di regressione $\beta_{ij}$ per ciascun gruppo:
\begin{itemize}	
\item Se i coefficienti $\beta_{0j}$ e $\beta_{ij}$ sono entrambi costanti allora la struttura
gerarchica non ha effetto, ovvero non vi è differenza con una regressione OLS;
\item Se i due coefficienti dipendono entrambe da $j$, ovvero dal gruppo di cui sono espressione, allora la regressione OLS
non può essere utilizzata.
\end{itemize}
Nello specifico riguardo quest'ultimo punto:
	\begin{enumerate}
	\item Se varia solo $\beta_{0j}$ con $j$ allora si ha un modello \textbf{Random Intercept};
 	\item Se anche $\beta_{ij}$ varia con $j$ allora il modello è detto \textbf{Random Coefficient}.
	\end{enumerate}
Ci sono diversi modi di approcciare lo studio della relazione tra $Y$ ed $X$:
\begin{itemize}
\item \textbf{Relazione diseggregata}: è una relazione in cui il raggruppamento delle unità (ovvero la sovrastruttura) viene ignorato
\begin{equation*}
y_{ij} = \beta_0 + \beta_1 x_{ij} +r_{ij} 
\end{equation*}
\item \textbf{Relazione aggregata fra i gruppi}: si può infatti essere interessati alla relazione aggregata, ovvero a livello macro, fra i gruppi, quindi alla relazione tra $\bar{x}_{.j}$ e $\bar{y}_{.j}$
\begin{equation*}
\bar{y}_{.j} = \beta_0 + \beta_1 \bar{x}_{.j} + r_j
\end{equation*}
\item \textbf{Relazione entro ciascun gruppo}: poichè si può essere interessati alla relazione tra $x_{ij}$ ed $y_{ij}$ entro ciascun gruppo $j$, ovvero a livello micro. Questo avviene assumendo $\alpha_1$ costante entro ciascun gruppo
\begin{equation*}
y_{ij} - \bar{y}_{.j} = \alpha_1(x_{ij} - \bar{x}_{.j}) + r
\end{equation*}
\item \textbf{Relazione Multilevel}: data la struttura dei dati, si può pensare di porre assieme la \textit{regressione tra i gruppi} e la \textit{regressione entro i gruppi} per cui $y_{ij}$ sarebbe funzione sia delle relazioni \textbf{entro} i gruppi, sia di quelle \textbf{tra} i gruppi.
\begin{equation*}
y_{ij} = \beta_0 + \beta_1 \bar{x}_{.j} +\alpha_1(x_{ij} - \bar{x}_{.j}) + r
\end{equation*} 
\end{itemize}

\section{Modello Multilevel: definizione e significato}
Per effettuare l'analisi della \textbf{covarianza} (\textbf{ANCOVA}) occorre prima di tutto partire da quello che è definito modello \textbf{ANOVA}, ovvero il modello di analisi della varianza:
\begin{equation*}
y_{ij} = \gamma_{00} + u_j +r_{ij}
\end{equation*}
con:
\begin{itemize}
\item $u_j$ uguale all'effetto dell'unità macro $j$, ovvero la differenza tra la media generale e la media del gruppo $j$;
\item $r_{ij}$ residuo relativo all'unità micro $i$ appartenente al gruppo $j$;
\item $\gamma_{00}+u_j$ media relativa al gruppo $j$;
\item $\gamma_{00}$ media di $y$ relativa all'intera popolazione.
\end{itemize}
Il modello di analisi della varianza ANOVA cerca di spiegare in che misura la variabilità della variabile dipendente $y$ è dovuta a differenze delle medie fra i gruppi. E' infatti il modello utilizzato in caso di stima di $y$ con sole covariate $x_j$ di tipo qualitativo.

Nel complesso dato:
\begin{center}
$y_{(1,n)} = [y_{1(1,n)},\dots,y_{p(1,n_p)}]$ \\
$r_{(1,n)}=[r_1,\dots,r_p)$ \\
con \  $(n_1+\dots+n_p) = n$
\end{center}
e costruendo $A$ matrice \textit{presenza-assenza}, composta solo di $0$ e $1$
\[A_{p,n}=
\begin{bmatrix}
1_{(n_{1},1)} & 0&0 \\
0&1_{(n_{2},1)}&0 \\
0&0&1_{(n_{p},1)}
\end{bmatrix}
\]
possiamo procedere a calcolare
\begin{center}
$u_1= u_1 A_1$ \\
$u_2 = u_2 A_2$ \\
$\dots \ \dots \ \dots$ \\
$u_p = u_p A_p$
\end{center}
ottenendo cosi per il gruppo $j$-esimo
\begin{equation*}
y_{j_{(1,n)}} -\gamma_{00_{(1,p)}} = u_{j_{(p,n)}} + r_{j_{(1,n)}}
\end{equation*}
\begin{equation*}
(y-\gamma_{00})(y-\gamma_{00})' = [uA-e][uA-e]' = uAAu'+rr'
\end{equation*}
Per avere un'idea della variabilità nei gruppi, su tutti i gruppi $j$ si considerano le devianze intra gruppo e, nell'ipotesi di omoschedasticità, essendo la varianza di ogni errore pari a $\sigma^2$ otteniamo che la devianza residua totale $SSR = n\sigma^2$.
\newline
\newline
Allo stesso modo la devianza tra i gruppi corrisponde alla devianza delle medie di gruppo perciò $SSE = \sum^p_{j=1} (u_j A_j)^2$.
\newline
\newline
Da cui devianza totale $SST = SSE + SSR$.
\newline
\newline
Definendo a questo punto
\begin{equation*}
\tau^2 = \frac{uAA'u'}{n}
\end{equation*}
\begin{equation*}
\sigma^2 = \frac{rr'}{n}
\end{equation*}
Otteniamo il \textbf{coefficiente di correlazione intraclasse} $\rho$
\begin{equation*}
\rho = \frac{\tau^2}{\tau^2 + \sigma^2}
\end{equation*}
ovvero il rapporto tra \textbf{devianze tra i gruppi} e \textbf{devianza complessiva}.

Questa struttura è fondamentale per ricondurre il modello ANOVA alla struttura del modello lineare con $$u = \beta$$ $$A = X$$ $$y-\gamma_{00}=y$$ $$r = \varepsilon$$

Il \textbf{modello ANCOVA} è un modello di analisi della \textbf{covarianza}: se infatti le caratteristiche $x$ delle osservazioni appartenenti ai diversi gruppi sono differenti tra gruppo e gruppo l'analisi della varianza viene distorta e si attribuiscono alla varianza fra i gruppi effetti che dipedono da tali caratteristiche. Occorre quindi prima di tutto eliminare l'effetto di queste caratteristiche sulla variabile dipendente attraverso regressione OLS e, successivamente, effettuare l'analisi della varianza depurata.

Ciò avviene attraverso 4 fasi:
\begin{enumerate}
\item Prima si procede al calcolo della devianza totale di $Y$;
\item Si stimano i coefficienti di regressione;
\item Si calcola la devianza spiegata di $X$ (SSE);
\item Infine si stima la devianza residua corretta di $y$ uguale a $SSR_{yc}=SST_y - SSE_x$.
\end{enumerate}
Successivamente, \textbf{l’analisi della varianza} (ANOVA) cattura la relazione aggregata fra i gruppi e quindi descrive la \textbf{varianza fra gruppi}.

Ciò avviene in 3 fasi:
\begin{enumerate}
\item Si effettua l'analisi della varianza su $SSR_{yc}$;
\item Si calcola la devianza spiegata del \textbf{fattore sperimentale} corretta per l'effetto della covariata $X$, ottenuta come $SSE_{yc} = SST_{yc}-SSR_{yc}$;
\item Nel complesso quindi si ottiene
\begin{equation*}
SST_y = SSE_x + SST_{yc} = SSE_x + SSE_{yc} + SSR_{yc}
\end{equation*}
\end{enumerate}
Questo modello quindi spezza in due l’analisi mettendo insieme la covarianza:
\begin{enumerate}
\item Elimina gli aspetti individuali; 
\item Analizza gli effetti di gruppo;
\item Attribuisce la varianza al gruppo di appartenenza.
\end{enumerate}
Il modello ANOVA è specificabile anche in una versione ad \textit{effetti casuali}, per cui abbiamo la stessa struttura citata in precedenza ma con $U_j$ ipotizzata variabile casuale con distribuzione $N(0, \tau^2)$ di cui $u_j$ è una particolare manifestazione e $E_{ij}$ un'altra variabile casuale con distribuzione $N(0,\sigma^2)$. In questo caso il modello è definito ad \textit{effetti misti}. 
Mentre quindi non cambia la forma, cambia la sostanza. 
Sotto il profilo interpretativo significa infatti che le medie parziali $u_j$ sono determinazioni di una variabile casuale $U_j$. In questo caso il Test F serve a verificare $H_0$ che le medie parziali ottenute dal campione possano essere ritenute nel complesso equivalenti, ma per confrontare tra loro le strutture di secondo livello, non si utilizzano più i valori delle medie campionarie, ma i loro intervalli di confidenza; ciò significa probabilizzare la gerarchia.

Nel caso si consideri l'intera popolazione, oppure $U_j$ ed $E_j$ abbiano distribuzione non normale è meglio utilizzare l'analisi della varianza ad effetti fissi.
\newline
\newline
Per concludere, dopo le lunghe premesse, il modello ANCOVA ad effetti variabili è appunto definito \textbf{modello Multilevel}: restando valide tutte le considerazioni riguardo il modello ANOVA ad effetti casuali in prima istanza la \textbf{regressione lineare} (OLS) cattura la relazione disaggregata tra i dati, così da eliminare l'effetto distorsivo sulla varianza tra i gruppi e ricavare la varianza nei gruppi, mentre l'analisi della varianza cattura la relazione aggregata fra i gruppi e descrive quindi la varianza fra  gruppi.

In un primo tipo di modelli (\textbf{Mixed Models}) la relazione disaggregata tra i dati e la varianza nei gruppi sono descritte mediante parametri fissi mentre la relazione aggregata fra i gruppi e la varianza fra gruppi sono descritte come variabili casuali.

In un secondo tipo di modelli (\textbf{Random Models}) anche la relazione disaggregata tra i dati e la varianza nei gruppi sono descritte come variabili casuali.

I modelli finora studiati possono essere visti come sottocasi del modello Multilevel:
\begin{enumerate}
\item Per $u_j=0$ e nessuna gerarchia dei dati, abbiamo un  \textbf{modello Lineare}: 
\begin{equation*}
y_i= \gamma_{00}+\sum_k \beta_k(x_{ik}-\bar{x}_{.k}) + \varepsilon_i \ ;
\end{equation*}
\item Per $u_j=0$ otteniamo una \textbf{regressione Multilevel}: 
\begin{equation*}
y_{ij} = \gamma_{00} + \sum_k \beta_k(x_{ijk}-\bar{x}_{.k}) + \varepsilon_{ij};
\end{equation*}
\item Per $\sum_k \beta_k (x_{ik} - \bar{x}_{.k}) = 0$ e $u_j$ fisso abbiamo un'\textbf{ANOVA}: 
\begin{equation*}
y_{ij} = \gamma_{00}+ u_j + \varepsilon_{ij};
\end{equation*}
\item Per $\sum_k\beta_k(x_{ik}- \bar{x}_{.k}) = 0$ e $u_j$ stocastico otteniamo un'\textbf{ANOVA ad effetti casuali}: 
\begin{equation*}
y_{ij} = \gamma_{00} + u_j + \varepsilon_{ij}.
\end{equation*}
\end{enumerate}
\section{Modello Multilevel: OLS, Empty, Mixed, Total Effects}

La stima del modello Multilevel si compone di 4 step:
\begin{enumerate}[noitemsep]
\item Si stima innanzitutto il modello lineare solitamente con il metodo di stima OLS;
\item Si propone poi l'Empty model ( anche detto \textbf{Unconditional means model UMM}) vale a dire l’analisi della varianza a effetti casuali.
\item Random intercepts model (RIM) cioè l’analisi della covarianza a effetti casuali per l’analisi della varianza.
\item Random slopes and intercepts model (UGM) analisi della covarianza a effetti casuali sia per il modello lineare che per l’analisi della varianza.
\end{enumerate}
\subsection*{1. Stima modello lineare con metodo di stima OLS}
Si consideri un modello Multilevel in cui appare solo la parte del modello Lineare che viene stimata mediante metodo OLS con una sola variabile e ipotizzando che le variabili $X$ e $Y$ siano centrate
\begin{equation*}
y_{ij}= \beta_0+\sum_{jk} \beta_k x_{ijk}+\varepsilon_{ij}
\end{equation*}
In questo modo si vede quale sia l’effetto delle variabili esplicative sulla variabile dipendente se i dati non fossero centrati. Naturalmente gli errori si distribuiscono come una normale.

Si possono proporre anche regressioni Multilevel introducendo variabili esplicative $Z$ misurate sui gruppi, e quindi rappresentanti il livello 2, invece che sugli individui. Questo aspetto inoltre può essere esteso anche all'interazione \textit{cross-level}, ciò significa che nel modello si possono introdurre variabili prodotto originate dall'interazione tra variabili misurate sull'individuo e misurate sui gruppi cui gli individui appartengono $z_w x_k$. Per risolvere la regressione multilevel si può scomporre il coefficiente di regressione in parte \textit{between} e parte \textit{within}. 

Il \textbf{modello di Cronbach} fa esattamente questo
\begin{equation*}
y_{ij} = \alpha +\beta_{within}(x_{ij}-\bar{x}_{.j}) +\beta_{between}\bar{x}_{.j}+\varepsilon
\end{equation*}
dove il \textit{contextual effect} $\delta$ è l'effetto della media dei gruppi che non è contemplato dal valore individuato
\begin{equation*}
\delta = \beta_{between}-\beta_{within}
\end{equation*}
dove $\beta_{between}-\beta_{within}$ sono gli effetti dovuti alla sovrastruttura al netto degli effetti individuali. 
\subsection*{2. Empty model}
Nel modello ANOVA ad effetti casuali detto anche \textbf{Empty model} si ha che
\begin{equation*}
y_{ij}=v_j+r_{ij}
\end{equation*}
In questo caso la variabile dipendente $y$ dipende dagli effetti casuali:
\begin{itemize}
\item a livello di gruppo, $V_j$, distribuiti in modo normale $N(\gamma_{00}, \tau^2)$
\item a livello individuale, dai residui $R_{ij}$, distribuiti in modo normale $N(0, \sigma^2)$ 
\end{itemize}
con $V_j$ ed $R_{ij}$ \textbf{indipendenti} e \textbf{mutualmente incorrelati}. \\
La variabilità all’interno di ogni gruppo è quindi dovuta solamente alla distribuzione casuale della variabile dipendente.

L’intercetta casuale a livello di gruppo può essere scomposta in due parti: l’intercetta fissa media tra tutti i gruppi $\gamma_{00}$ e la misura della sua deviazione attorno alla media tra i gruppi di tipo casuale $u_j$
\begin{equation*}
v_j=\gamma_{00}+u_j
\end{equation*}
Possiamo a questo punto riscrivere il modello nel seguente modo: 
\begin{equation*}
y_{ij}=\gamma_{00}+u_{j}+r_{ij}
\end{equation*}
In questo modello quindi la variabilità totale di $y$ può essere scomposta nella somma delle varianze ai due livelli, varianza fra i gruppi e varianza nei gruppi
\begin{equation*}
Var(y)=Var(U_j)+Var(R_{ij})=\tau^2+\sigma^2
\end{equation*}
Si può quindi definire il coefficiente di correlazione intraclasse, come visto in precedenza
\begin{equation*}
\rho = \frac{\tau^2}{\tau^2+\sigma^2}
\end{equation*} 
Il coefficiente di correlazione intraclasse $\rho$ misura quindi la quota di varianza di $y$ spiegata dall’appartenenza ai gruppi dei singoli individui. Se $\rho = 0$, ovvero tutti gli $u_j$ sono nulli, allora il raggruppamento è irrilevante ed è inutile utilizzare altri modelli oltre il modello lineare semplice. Nel caso invece $\rho$ fosse positivo, è necessario considerare un modello di tipo gerarchico.

Il Test $F$ come in ogni analisi della varianza può essere utilizzato per verificare in termini inferenziali l’ipotesi che le intercette casuali $u_j$ siano nel complesso tra loro equivalenti (nel caso non ci fosse differenza fra gruppi). In questo caso il Test $F$ serve per capire se nel complesso vale l’ipotesi nulla $H_0$ che le medie parziali ottenute nel campione possano essere ritenute nel complesso equivalenti. Per confrontare tra loro le strutture di secondo livello (ad esempio scuole, ospedali, università) come nell’analisi della varianza casuale non si utilizzano i valori delle medie campionarie, non informative del vero valore di $U_j$ ma i loro \textit{intervalli di confidenza} che comprendono con una probabilità del 90\%, 95\%, 99\% i valori veri ignoti di $U_j$.
Ciò significa probabilizzare la gerarchia fra medie parziali in quanto, tanto più sono piccoli gli intervalli di confidenza, maggiore è la loro capacità di fornire informazioni sui valori veri ignoti di $U_j$. 
\newline
\newline
\textbf{NB} A differenza dell’analisi della varianza casuale, nel Modello Multilevel che è, come ricordato, un’analisi della \textit{covarianza casuale}, tali intervalli di confidenza sono \textbf{al netto dell’influenza delle variabili $X$ del modello lineare}. Questa probabilizzazione della gerarchia influenza e rende più robusto il confronto fra strutture di secondo livello in quanto una media parziale $u_j$ di una struttura $J$ si considera superiore a un’altra media parziale $u_g$ di una struttura $G$ se e solo se l’estremo inferiore del suo intervallo di confidenza è più grande dell’estremo superiore dell’altra in quanto, solo in questo caso, con un elevato grado di probabilità, il valore vero di $J$ sarà più grande di $G$.

\subsection*{3. Random intercept model}
Se si inserisce nel modello Empty una variabile esplicativa $x_k$ il modello diventa il vero e proprio random intercept model (mixed model) 
\begin{equation*}
y_{ij}=\gamma_{00}+\beta_1 x_{ij}+ u_j + \varepsilon_{ij} 
\end{equation*}
dove $u_j$ è la determinazione della \textit{variabile casuale} $U_j$ distribuita normalmente $N(\gamma_{00}, \tau^2)$ a rappresentazione dei residui di secondo livello. Ess sono indipendenti e quindi incorrelati con i residui di primo livello $\varepsilon_{ij}$ determinazioni della variabile casuale normalmente distribuita $E_{ij} \sim N(0,\sigma^2)$

In questo caso la variabile dipendente $y$ dipende:
\begin{itemize}
\item dalle variabili $X$ e dai relativi parametri fissi $\beta$;
\item dall’effetto casuale a livello di gruppo $u_j$, che si distribuisce in modo normale  $N(\gamma_{00}, \tau^2)$;
\item dall’effetto casuale a livello individuale $\varepsilon_{ij}$, che si distribuisce in modo normale $N(0,\sigma^2)$.
\end{itemize}
La correlazione intraclasse $\rho$ misura la quota di varianza di $y$ spiegata dall’appartenenza ai gruppi dei singoli individui, \textbf{al netto della quota di varianza spiegata da $x$} (a differenza del modello Empty, in questa circostanza ho $X$ che spiega una parte della variabilità non dovuta all’appartenenza a un gruppo di un individuo). Per questa ragione il suo valore può decrescere anche molto dal caso rispetto al caso del modello Empty. 

Il modello comprende 4 parametri da stimare:
\begin{itemize}
\item i coefficienti di regressione $\gamma_{00}$ e $\beta$; \item le componenti della varianza $\sigma^{2}$ e $\tau^{2}$.
\end{itemize}
Il coefficiente di regressione $\beta$ può essere interpretato come variazione di $Y$ corrispondente ad una variazione unitaria di $X$. In un modello di regressione semplice la variabilità di $Y$ non spiegata dalla regressione è semplicemente data dai residui $\varepsilon_{ij}$.

La variabilità in un modello multilevel, invece, fa riferimento a più popolazioni:
\begin{itemize}
\item La v.c. $U_j$ può essere vista come le variabile casuale che descrive i residui a livello di gruppo, ovvero gli effetti di gruppo non spiegati da $X$;
\item La v.c. $E_{ij}$ può essere vista come variabile casuale che descrive i residui a livello di individuo, ovvero gli effetti individuali non spiegati da $X$.
\end{itemize}
\subsubsection*{Rappresentazioni di un modello random intercept}
\begin{itemize}
\item Micro model: $y_{ij}=\beta_1 x_{ij}+R_{ij}$
\item Macro model: $\beta_{0j}=\gamma_{00}+U_{0j}$
\end{itemize}
Come unica equazione multilevel
\begin{equation*}
y_{ij}=\gamma_{00}+\beta_1 x_{ij}+ U_{0j}+R_{ij}
\end{equation*}
Con
\begin{itemize}
\item parte fissa del modello $y_{ij}=\gamma_{00}+\beta_1 x_{ij}$
\item parte casuale (random part) del modello $U_{0j}+ R_{ij}$
\end{itemize}
e come varianze e covarianze al primo livello $\sigma^2$, ed al secondo livello $\tau^2$.
\newline
\newline
\textbf{NB} Variabili esplicative relative al secondo livello possono essere misurate direttamente sulle unità di secondo livello o derivate dalle misurazioni effettuate sulle unità di primo livello. Inoltre spesso la numerosità dei vari gruppi è diversa da gruppo a gruppo, questo però non rappresenta un problema grazie all'\textbf{effetto shrinkage}, che tiene conto della diversa numerosità dei gruppi facendo pesare di più per la stima complessiva, i gruppi più numerosi. 
\subsection*{4. Random slopes and intercepts model} 
La relazione tra variabile dipendente $Y$ e variabili esplicative $X_j$ può variare tra i gruppi in modi diversi: si può infatti avere un’eterogeneità delle regressioni tra i diversi gruppi (si parla anche di interazione gruppo – covariate). 

Ad esempio nel caso dell’analisi delle performance degli studenti appartenenti alle scuole, si può assumere che l’effetto dello stato socio economico o dell’intelligenza individuale sulle performance possa essere diverso nelle singole scuole.

La struttura dei dati ed il loro raggruppamento può essere spiegato quindi anche facendo variare i coefficienti della regressione da gruppo a gruppo.
\begin{equation*}
y_{ij}=\beta_{0j}+\beta_{1j} x_{ij}+ R_{ij}
\end{equation*}
A seconda del comportamento di $\beta_{0j}$ e $\beta_{1j}$ possiamo ottenere:
\begin{itemize}
\item Con diversi $\beta_{0j}$ in base ai gruppi otteniamo un  \textbf{modello Random Intercept};
\item Con anche diversi $\beta_{1j}$ in base ai gruppi otteniamo un \textbf{modello Random Coefficient};
\item Se i coefficienti $\beta_{0j}$ e $\beta_{1j}$ sono entrambi costanti la struttura gerarchica non ha effetto ed otteniamo un \textbf{modello OLS}.
\end{itemize}
Considerando
\begin{equation*}
y_{ij} = \beta_{0j} + \beta_{1j} x_{ij} +R_{ij}
\end{equation*}
con $\beta_{0j}$ e $\beta_{1j}$ variabili, ognuno può essere scomposto in parte costante e deviazione dalla media a livello di gruppo:
\begin{center}
$\beta_{0j} = \gamma_{00} + U_{0j}$ \\
$\beta_{1j} = \gamma_{10} + U_{1j}$
\end{center}
ottenendo quindi l'equazione completa
\begin{equation*}
y_{ij} = \gamma_{00} + \gamma_{10} x_{ij} + U_{0j} + U_{1j} x_{ij} + R_{ij}
\end{equation*}
con effetti di gruppo dati da
\begin{itemize}
\item $U_{0j}$ intercetta random;
\item $U_{1j} x_{ij}$ interazione random tra i gruppi e la variabile esplicativa $x_{ij}$;
\item $\gamma_{00} + \gamma_{10} x_{ij}$ parte fissa del modello generale;
\item $U_{0j} + U_{10} x_{ij} + R_{ij}$ parte random del modello generale.
\end{itemize}
\section{Metodi di stima e verifica di ipotesi}
\subsection*{Specificazione del modello e stima dei parametri}
La specificazione del modello comporta la scelta del modello più soddisfacente. Nel caso di modelli lineari gerarchici tutto ciò implica:
\begin{itemize}
\item Scelta delle variabili esplicative $x_j$ e delle interazioni della parte fissa;
\item Scelta dei coefficienti casuali con le strutture di covarianza per la parte random del modello.
\end{itemize}
I parametri da stimare nel modello random intercept sono:
\begin{itemize}
\item Coefficienti di regressione $\gamma_{00}$ e $\beta$;
\item Componenti di varianza, $\sigma^2$ e $\tau^2$; 
\item Gli effetti casuali $U_{0j}$ non sono parametri ma variabili casuali latenti, ovvero non direttamente osservabili. 
\end{itemize}
I metodi comunemente utilizzati per la stima dei parametri sotto l’assunzione che i residui $U_{0j}$ e $R_{ij}$ siano distribuiti normalmente sono il metodo del \textit{maximum likelihood} (ML) ed il \textit{restricted maximum likelihood} (REML).

Il metodo REML massimizza la verosimiglianza (likelihood) dei residui osservati ottenendo le stime degli effetti fissi usando metodi «non likelihood-like» come \textit{ordinary least squares} (OLS) o \textit{generalized least squares} (GLS) e, successivamente usa queste per massimizzare la verosimiglianza dei residui (sottraendo gli effetti misti) per ottenere le stime dei parametri della varianza.
\subsection*{Verifica di ipotesi}
\subsubsection*{Test sui parametri fissi del modello}
Per testare i parametri fissi del modello si utilizza la seguente ipotesi nulla (ipotesi di significatività) su ciascun parametro 
\begin{equation*}
H_0: \gamma_h = 0
\end{equation*}
\newline
Questa ipotesi viene verificata con un Test t
\begin{equation*}
\mathrm{T}(\gamma_h) = \frac{\hat{\gamma}_h}{\mathrm{s.e.(\hat{\gamma}_h)}}
\end{equation*}
noto come WALD TEST. 

Sotto l’ipotesi nulla il test ha approssimativamente una distribuzione t con g.d.l. basati sulla struttura multilevel dell’analisi. 
\subsubsection*{Test su più parametri della parte fissa del modello e parte random}
Per testare più parametri (fissi e random) del modello invece viene utilizzato il deviance test. 

Dalla stima del modello lineare con il metodo ML si ottiene la verosimiglianza del modello, da cui: 
\begin{equation*}
\mathrm{DEVIANCE} = -2 \cdot ln(\mathrm{Likelihood}) 
\end{equation*}
misura della bontà di adattamento ai dati del modello.

Solitamente la deviance viene interpretata in termini differenziali, ovvero si calcola la differenza tra le deviance di modelli alternativi. 

Si tratta di confrontare i valori osservati della variabile dipendente con i valori teorici di due modelli: 
\begin{enumerate}[noitemsep]
\item l'uno con le variabili esplicative di interesse e l’altro senza alcuna variabile (\textbf{empty-model}); 
\item l'uno con le variabili esplicative di interesse e l’altro che contiene “tanti parametri quante sono le osservazioni” (\textbf{saturated model}).
\end{enumerate}
Il confronto si basa sulla funzione di log-verosimiglianza: perciò indicate rispettivamente con $D_0$, $D_{mod}$, $D_{sat}$ le devianze calcolate per il \textbf{modello vuoto} (empty-model), il \textbf{modello considerato} e il \textbf{modello saturo}, valori di $D_{mod}$ più prossimi a $0$ che non a $D_0$ faranno propendere per ritenere “buono” il modello considerato. 

Ognuna delle devianze ha distribuzione asintotica $\chi^2$ (con $j$ gradi di libertà pari al numero delle variabili esplicative). Le loro differenze avranno distribuzione $\chi^2$ (con k gradi di libertà pari alla differenza del numero di variabili esplicative).

Se l’obiettivo è quello di sottoporre a verifica l’ipotesi che riguarda la nullità congiunta di tutti i coefficienti (esclusa l’intercetta),
\begin{equation*}
H_0 : \beta_1 = \beta_2 = \dots = 0
\end{equation*}
si può pensare a ragion veduta di operare un confronto fra due modelli: l’empty-model e il modello ipotizzato. 
Questo test può essere applicato sia alla parte fissa sia a quella random del modello.
\newpage
\part*{Dimostrazioni}
\section*{Non Efficienza con Errori Eteroschedastici}
Siano $\varepsilon^*$ gli errori eteroschedastici,  $E[b^*]=E[((X'X)^{-1}X'y-b)((X'X^{-1})X'y-b)'] =$\newline  $E[((X'X)^{-1}X'(X\bcancel{b}+\varepsilon^*)-\bcancel{b})((X'X^{-1})X'(X\bcancel{b}+\varepsilon^*)-\bcancel{b})'] = (X'X)^{-1}X'E[\varepsilon^* (\varepsilon^*)']((X'X)^{-1}X')' = \newline (X'X)^{-1}X' \Sigma_{\varepsilon^*}((X'X)^{-1}X')'$ e abbiamo che $\Sigma_{\varepsilon^*}$ varia al variare dell'indice. $ \hfill \square$

\section*{Non collinarietà dopo la trasformazione ritardata}
Il modello trasformato è 
\[y^{\#}_t = \beta_{0} + \beta_1x_t^{\#} + w_t \] 
quindi abbiamo che 
\[ cov(w_t,w_{t-1}) = cov(\varepsilon_t^{\#} - \rho \varepsilon_{t-1}^\#, \varepsilon_{t-1}^\# - \rho \varepsilon_{t-2}^\#) =  \]
\[ = cov(\varepsilon_{t}^\#,\varepsilon_{t-1}^\#) -\rho cov(\varepsilon_{t-1}^\#,\varepsilon_{t-1}^\#) - \rho cov(\varepsilon_{t}^\#,\varepsilon_{t-2}^\#) + \rho^2 cov(\varepsilon_{t-1}^\#,\varepsilon_{t-2}^\#) = \]
\[ = \rho - \rho - \rho^3 + \rho^3  = 0 \]
il risulta poteva anche essere intuitivo poiché ho eliminato la parte ritardata che era presente nella variabile e portava alla multicollinarietà. $\hfill \square$

\end{document}